{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import numpy, textblob, string\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy as sp\n",
    "import xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44183 entries, 0 to 44182\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Comment  44183 non-null  object\n",
      " 1   Outcome  44183 non-null  int64 \n",
      " 2   Id       44183 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = ''\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(stems)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "      <td>1</td>\n",
       "      <td>2994</td>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>22730</td>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can drop any row containing a missing usin...</td>\n",
       "      <td>1</td>\n",
       "      <td>49407</td>\n",
       "      <td>you can drop any row containing a missing usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you need to use strptime() to convert the stri...</td>\n",
       "      <td>1</td>\n",
       "      <td>26239</td>\n",
       "      <td>you need to use strptime() to convert the stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm no r expert, but most languages use a refe...</td>\n",
       "      <td>1</td>\n",
       "      <td>35866</td>\n",
       "      <td>I am no r expert, but most languages use a ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i don't know r at all, but a bit of creative g...</td>\n",
       "      <td>1</td>\n",
       "      <td>19528</td>\n",
       "      <td>i do not know r at all, but a bit of creative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if you don't want to modify the list in-place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>36784</td>\n",
       "      <td>if you do not want to modify the list in-place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "      <td>1</td>\n",
       "      <td>22146</td>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if you're willing to entertain an alternate pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>48389</td>\n",
       "      <td>if you are willing to entertain an alternate p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see ?order. you just need the last index (or f...</td>\n",
       "      <td>1</td>\n",
       "      <td>36037</td>\n",
       "      <td>see ?order. you just need the last index (or f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Outcome     Id  \\\n",
       "0  combining lindelof's and gregg lind's ideas: l...        1   2994   \n",
       "1  in most cases r is an interpreted language tha...        1  22730   \n",
       "2  you can drop any row containing a missing usin...        1  49407   \n",
       "3  you need to use strptime() to convert the stri...        1  26239   \n",
       "4  i'm no r expert, but most languages use a refe...        1  35866   \n",
       "5  i don't know r at all, but a bit of creative g...        1  19528   \n",
       "6  if you don't want to modify the list in-place ...        1  36784   \n",
       "7  i assume it helps if the matrix is sparse? yes...        1  22146   \n",
       "8  if you're willing to entertain an alternate pl...        1  48389   \n",
       "9  see ?order. you just need the last index (or f...        1  36037   \n",
       "\n",
       "                                               clean  \n",
       "0  combining lindelof's and gregg lind's ideas: l...  \n",
       "1  in most cases r is an interpreted language tha...  \n",
       "2  you can drop any row containing a missing usin...  \n",
       "3  you need to use strptime() to convert the stri...  \n",
       "4  I am no r expert, but most languages use a ref...  \n",
       "5  i do not know r at all, but a bit of creative ...  \n",
       "6  if you do not want to modify the list in-place...  \n",
       "7  i assume it helps if the matrix is sparse? yes...  \n",
       "8  if you are willing to entertain an alternate p...  \n",
       "9  see ?order. you just need the last index (or f...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Denoise text: Replace contradtions\n",
    "df_train[\"clean\"] = df_train.Comment.apply(replace_contractions)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "      <td>1</td>\n",
       "      <td>2994</td>\n",
       "      <td>[combining, lindelof, 's, and, gregg, lind, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>22730</td>\n",
       "      <td>[in, most, cases, r, is, an, interpreted, lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can drop any row containing a missing usin...</td>\n",
       "      <td>1</td>\n",
       "      <td>49407</td>\n",
       "      <td>[you, can, drop, any, row, containing, a, miss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you need to use strptime() to convert the stri...</td>\n",
       "      <td>1</td>\n",
       "      <td>26239</td>\n",
       "      <td>[you, need, to, use, strptime, (, ), to, conve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm no r expert, but most languages use a refe...</td>\n",
       "      <td>1</td>\n",
       "      <td>35866</td>\n",
       "      <td>[I, am, no, r, expert, ,, but, most, languages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i don't know r at all, but a bit of creative g...</td>\n",
       "      <td>1</td>\n",
       "      <td>19528</td>\n",
       "      <td>[i, do, not, know, r, at, all, ,, but, a, bit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if you don't want to modify the list in-place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>36784</td>\n",
       "      <td>[if, you, do, not, want, to, modify, the, list...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "      <td>1</td>\n",
       "      <td>22146</td>\n",
       "      <td>[i, assume, it, helps, if, the, matrix, is, sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if you're willing to entertain an alternate pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>48389</td>\n",
       "      <td>[if, you, are, willing, to, entertain, an, alt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see ?order. you just need the last index (or f...</td>\n",
       "      <td>1</td>\n",
       "      <td>36037</td>\n",
       "      <td>[see, ?, order, ., you, just, need, the, last,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Outcome     Id  \\\n",
       "0  combining lindelof's and gregg lind's ideas: l...        1   2994   \n",
       "1  in most cases r is an interpreted language tha...        1  22730   \n",
       "2  you can drop any row containing a missing usin...        1  49407   \n",
       "3  you need to use strptime() to convert the stri...        1  26239   \n",
       "4  i'm no r expert, but most languages use a refe...        1  35866   \n",
       "5  i don't know r at all, but a bit of creative g...        1  19528   \n",
       "6  if you don't want to modify the list in-place ...        1  36784   \n",
       "7  i assume it helps if the matrix is sparse? yes...        1  22146   \n",
       "8  if you're willing to entertain an alternate pl...        1  48389   \n",
       "9  see ?order. you just need the last index (or f...        1  36037   \n",
       "\n",
       "                                               clean  \n",
       "0  [combining, lindelof, 's, and, gregg, lind, 's...  \n",
       "1  [in, most, cases, r, is, an, interpreted, lang...  \n",
       "2  [you, can, drop, any, row, containing, a, miss...  \n",
       "3  [you, need, to, use, strptime, (, ), to, conve...  \n",
       "4  [I, am, no, r, expert, ,, but, most, languages...  \n",
       "5  [i, do, not, know, r, at, all, ,, but, a, bit,...  \n",
       "6  [if, you, do, not, want, to, modify, the, list...  \n",
       "7  [i, assume, it, helps, if, the, matrix, is, sp...  \n",
       "8  [if, you, are, willing, to, entertain, an, alt...  \n",
       "9  [see, ?, order, ., you, just, need, the, last,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text: Using nltk packages\n",
    "df_train[\"clean\"]  = df_train[\"clean\"].apply(nltk.word_tokenize)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "      <td>1</td>\n",
       "      <td>2994</td>\n",
       "      <td>[combining, lindelof, gregg, lind, ideas, last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>22730</td>\n",
       "      <td>[cases, r, interpreted, language, runs, readev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can drop any row containing a missing usin...</td>\n",
       "      <td>1</td>\n",
       "      <td>49407</td>\n",
       "      <td>[drop, row, containing, missing, using, naomit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you need to use strptime() to convert the stri...</td>\n",
       "      <td>1</td>\n",
       "      <td>26239</td>\n",
       "      <td>[need, use, strptime, convert, string, date, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm no r expert, but most languages use a refe...</td>\n",
       "      <td>1</td>\n",
       "      <td>35866</td>\n",
       "      <td>[r, expert, languages, use, reference, countin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i don't know r at all, but a bit of creative g...</td>\n",
       "      <td>1</td>\n",
       "      <td>19528</td>\n",
       "      <td>[know, r, bit, creative, googling, led, http, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if you don't want to modify the list in-place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>36784</td>\n",
       "      <td>[want, modify, list, inplace, eg, passing, lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "      <td>1</td>\n",
       "      <td>22146</td>\n",
       "      <td>[assume, helps, matrix, sparse, yes, algorithm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if you're willing to entertain an alternate pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>48389</td>\n",
       "      <td>[willing, entertain, alternate, plotting, pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see ?order. you just need the last index (or f...</td>\n",
       "      <td>1</td>\n",
       "      <td>36037</td>\n",
       "      <td>[see, order, need, last, index, first, decreas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Outcome     Id  \\\n",
       "0  combining lindelof's and gregg lind's ideas: l...        1   2994   \n",
       "1  in most cases r is an interpreted language tha...        1  22730   \n",
       "2  you can drop any row containing a missing usin...        1  49407   \n",
       "3  you need to use strptime() to convert the stri...        1  26239   \n",
       "4  i'm no r expert, but most languages use a refe...        1  35866   \n",
       "5  i don't know r at all, but a bit of creative g...        1  19528   \n",
       "6  if you don't want to modify the list in-place ...        1  36784   \n",
       "7  i assume it helps if the matrix is sparse? yes...        1  22146   \n",
       "8  if you're willing to entertain an alternate pl...        1  48389   \n",
       "9  see ?order. you just need the last index (or f...        1  36037   \n",
       "\n",
       "                                               clean  \n",
       "0  [combining, lindelof, gregg, lind, ideas, last...  \n",
       "1  [cases, r, interpreted, language, runs, readev...  \n",
       "2  [drop, row, containing, missing, using, naomit...  \n",
       "3  [need, use, strptime, convert, string, date, e...  \n",
       "4  [r, expert, languages, use, reference, countin...  \n",
       "5  [know, r, bit, creative, googling, led, http, ...  \n",
       "6  [want, modify, list, inplace, eg, passing, lis...  \n",
       "7  [assume, helps, matrix, sparse, yes, algorithm...  \n",
       "8  [willing, entertain, alternate, plotting, pack...  \n",
       "9  [see, order, need, last, index, first, decreas...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the list of words (tokens)\n",
    "df_train[\"clean\"]  = df_train[\"clean\"].apply(normalize)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "      <td>1</td>\n",
       "      <td>2994</td>\n",
       "      <td>[combin, lindelof, greg, lind, idea, last, fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>22730</td>\n",
       "      <td>[cas, r, interpret, langu, run, readevaluatepr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can drop any row containing a missing usin...</td>\n",
       "      <td>1</td>\n",
       "      <td>49407</td>\n",
       "      <td>[drop, row, contain, miss, us, naomit, howev, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you need to use strptime() to convert the stri...</td>\n",
       "      <td>1</td>\n",
       "      <td>26239</td>\n",
       "      <td>[nee, us, strptime, convert, string, dat, exam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm no r expert, but most languages use a refe...</td>\n",
       "      <td>1</td>\n",
       "      <td>35866</td>\n",
       "      <td>[r, expert, langu, us, ref, count, scheme, big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i don't know r at all, but a bit of creative g...</td>\n",
       "      <td>1</td>\n",
       "      <td>19528</td>\n",
       "      <td>[know, r, bite, cre, googl, lead, http, tolsto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if you don't want to modify the list in-place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>36784</td>\n",
       "      <td>[want, mod, list, inplac, eg, pass, list, el, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "      <td>1</td>\n",
       "      <td>22146</td>\n",
       "      <td>[assum, help, matrix, spar, ye, algorithm, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if you're willing to entertain an alternate pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>48389</td>\n",
       "      <td>[wil, entertain, altern, plot, pack, ggplot2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see ?order. you just need the last index (or f...</td>\n",
       "      <td>1</td>\n",
       "      <td>36037</td>\n",
       "      <td>[see, ord, nee, last, index, first, decreas, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Outcome     Id  \\\n",
       "0  combining lindelof's and gregg lind's ideas: l...        1   2994   \n",
       "1  in most cases r is an interpreted language tha...        1  22730   \n",
       "2  you can drop any row containing a missing usin...        1  49407   \n",
       "3  you need to use strptime() to convert the stri...        1  26239   \n",
       "4  i'm no r expert, but most languages use a refe...        1  35866   \n",
       "5  i don't know r at all, but a bit of creative g...        1  19528   \n",
       "6  if you don't want to modify the list in-place ...        1  36784   \n",
       "7  i assume it helps if the matrix is sparse? yes...        1  22146   \n",
       "8  if you're willing to entertain an alternate pl...        1  48389   \n",
       "9  see ?order. you just need the last index (or f...        1  36037   \n",
       "\n",
       "                                               clean  \n",
       "0  [combin, lindelof, greg, lind, idea, last, fun...  \n",
       "1  [cas, r, interpret, langu, run, readevaluatepr...  \n",
       "2  [drop, row, contain, miss, us, naomit, howev, ...  \n",
       "3  [nee, us, strptime, convert, string, dat, exam...  \n",
       "4  [r, expert, langu, us, ref, count, scheme, big...  \n",
       "5  [know, r, bite, cre, googl, lead, http, tolsto...  \n",
       "6  [want, mod, list, inplac, eg, pass, list, el, ...  \n",
       "7  [assum, help, matrix, spar, ye, algorithm, per...  \n",
       "8  [wil, entertain, altern, plot, pack, ggplot2, ...  \n",
       "9  [see, ord, nee, last, index, first, decreas, o...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem/Lementize the list of words (tokens)\n",
    "df_train[\"clean\"]  = df_train[\"clean\"].apply(stem_and_lemmatize)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "      <td>1</td>\n",
       "      <td>2994</td>\n",
       "      <td>combin lindelof greg lind idea last funct x ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>22730</td>\n",
       "      <td>cas r interpret langu run readevaluateprint lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can drop any row containing a missing usin...</td>\n",
       "      <td>1</td>\n",
       "      <td>49407</td>\n",
       "      <td>drop row contain miss us naomit howev want mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you need to use strptime() to convert the stri...</td>\n",
       "      <td>1</td>\n",
       "      <td>26239</td>\n",
       "      <td>nee us strptime convert string dat exampl strp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm no r expert, but most languages use a refe...</td>\n",
       "      <td>1</td>\n",
       "      <td>35866</td>\n",
       "      <td>r expert langu us ref count scheme big object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i don't know r at all, but a bit of creative g...</td>\n",
       "      <td>1</td>\n",
       "      <td>19528</td>\n",
       "      <td>know r bite cre googl lead http tolstoynewcast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if you don't want to modify the list in-place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>36784</td>\n",
       "      <td>want mod list inplac eg pass list el remov fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "      <td>1</td>\n",
       "      <td>22146</td>\n",
       "      <td>assum help matrix spar ye algorithm perform we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if you're willing to entertain an alternate pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>48389</td>\n",
       "      <td>wil entertain altern plot pack ggplot2 autom s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see ?order. you just need the last index (or f...</td>\n",
       "      <td>1</td>\n",
       "      <td>36037</td>\n",
       "      <td>see ord nee last index first decreas ord trick...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Outcome     Id  \\\n",
       "0  combining lindelof's and gregg lind's ideas: l...        1   2994   \n",
       "1  in most cases r is an interpreted language tha...        1  22730   \n",
       "2  you can drop any row containing a missing usin...        1  49407   \n",
       "3  you need to use strptime() to convert the stri...        1  26239   \n",
       "4  i'm no r expert, but most languages use a refe...        1  35866   \n",
       "5  i don't know r at all, but a bit of creative g...        1  19528   \n",
       "6  if you don't want to modify the list in-place ...        1  36784   \n",
       "7  i assume it helps if the matrix is sparse? yes...        1  22146   \n",
       "8  if you're willing to entertain an alternate pl...        1  48389   \n",
       "9  see ?order. you just need the last index (or f...        1  36037   \n",
       "\n",
       "                                               clean  \n",
       "0  combin lindelof greg lind idea last funct x ta...  \n",
       "1  cas r interpret langu run readevaluateprint lo...  \n",
       "2  drop row contain miss us naomit howev want mor...  \n",
       "3  nee us strptime convert string dat exampl strp...  \n",
       "4  r expert langu us ref count scheme big object ...  \n",
       "5  know r bite cre googl lead http tolstoynewcast...  \n",
       "6  want mod list inplac eg pass list el remov fun...  \n",
       "7  assum help matrix spar ye algorithm perform we...  \n",
       "8  wil entertain altern plot pack ggplot2 autom s...  \n",
       "9  see ord nee last index first decreas ord trick...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detokenize the list of words back to text\n",
    "df_train[\"clean\"]  = df_train[\"clean\"].apply(TreebankWordDetokenizer().detokenize)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing train and test splition first:\n",
    "# Notice that this is just a simple train test split not CV, since I am just going to use this validation result to \n",
    "# specify which features to use and which models to tune in the finalized infrustructure: not final\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_train[\"clean\"], df_train[\"Outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count Vector features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df_train[\"clean\"])\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33137x158473 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1180663 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11046x158473 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 392937 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# 2. TF-IDF features\n",
    "# With 3 levels:\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df_train[\"clean\"])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(df_train[\"clean\"])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(df_train[\"clean\"])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>435</td>\n",
       "      <td>80</td>\n",
       "      <td>5.370370</td>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>39</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>584</td>\n",
       "      <td>113</td>\n",
       "      <td>5.122807</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>436</td>\n",
       "      <td>67</td>\n",
       "      <td>6.411765</td>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "      <td>49</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>894</td>\n",
       "      <td>164</td>\n",
       "      <td>5.418182</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>567</td>\n",
       "      <td>102</td>\n",
       "      <td>5.504854</td>\n",
       "      <td>92</td>\n",
       "      <td>54</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>23</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>377</td>\n",
       "      <td>63</td>\n",
       "      <td>5.890625</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>20</td>\n",
       "      <td>6.476190</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outcome  char_count  word_count  word_density  punctuation_count  \\\n",
       "0        1         435          80      5.370370                 51   \n",
       "1        1         234          39      5.850000                  4   \n",
       "2        1         584         113      5.122807                 65   \n",
       "3        1         436          67      6.411765                 52   \n",
       "4        1         254          49      5.080000                  9   \n",
       "5        1         894         164      5.418182                 49   \n",
       "6        1         567         102      5.504854                 92   \n",
       "7        1         165          23      6.875000                 14   \n",
       "8        1         377          63      5.890625                 18   \n",
       "9        1         136          20      6.476190                 16   \n",
       "\n",
       "   noun_count  verb_count  adj_count  adv_count  pron_count  \n",
       "0          30           9          9          6           1  \n",
       "1          10           8          7          1           0  \n",
       "2          27          27         10          6           6  \n",
       "3          34          16          1          6           5  \n",
       "4          18           8          2          3           2  \n",
       "5          49          30         14         10           4  \n",
       "6          54          18          8          6           2  \n",
       "7           9           6          0          2           1  \n",
       "8          19          13          4          6           3  \n",
       "9          10           5          2          2           1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Text / NLP based features\n",
    "# create more useful features:\n",
    "df_create = df_train[['Outcome']]\n",
    "df_create['char_count'] = df_train['Comment'].apply(len)\n",
    "df_create['word_count'] = df_train['Comment'].apply(lambda x: len(x.split()))\n",
    "df_create['word_density'] = df_create['char_count'] / (df_create['word_count']+1)\n",
    "df_create['punctuation_count'] = df_train['Comment'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "df_create['noun_count'] = df_train['Comment'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "df_create['verb_count'] = df_train['Comment'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "df_create['adj_count'] = df_train['Comment'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "df_create['adv_count'] = df_train['Comment'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "df_create['pron_count'] = df_train['Comment'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "df_create.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data aes plot ggplot label group library x ggplot2 c',\n",
       " 'model data fit import x cluster plot predict train 0000',\n",
       " 'func window def server address stream connect port tkinter mysql',\n",
       " 'path directory file root folder project windows fn src ospathjoin',\n",
       " 'listpl3d7bff1ddbdaafe5 usa unite slope sales key2 population temperature outliers anaconda',\n",
       " 'plot text word color col eval h w blue mtcars',\n",
       " 'django python app language model database application user obj password',\n",
       " 'date year format datetime df plyr asdate days offset echo',\n",
       " 'p array k numpy q point import ax shape decimal',\n",
       " 'input user template a1 virtualenv output flask view elapse 000',\n",
       " 'install package http url pip request instal import python https',\n",
       " 'int encode item unicode utf8 title expr bytes ascii microbenchmark',\n",
       " 'mydata message member msg country blah import sign cursor params',\n",
       " 'false true key none value cat return node xml td',\n",
       " 'form product t1 txt ratio rank queryset polygons time relationship',\n",
       " 'use python return function print def code class object need',\n",
       " 'image chr table struct username frame 0 255 2l img',\n",
       " '1 2 x 0 3 c 4 5 b df',\n",
       " 'lambda month mylist betterbar filter 09 az parent child aa',\n",
       " 'file line import open f print read process r filename']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choice of Models & Train Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "    \n",
    "    if is_neural_net:\n",
    "    # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label, epochs=3)\n",
    "    # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.6571609632446135\n",
      "NB, WordLevel TF-IDF:  0.6633170378417527\n",
      "NB, N-Gram Vectors:  0.6388738004707586\n",
      "NB, CharLevel Vectors:  0.6377874343653811\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.678164041281912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.6938258193011044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors:  0.6577946768060836\n",
      "LR, CharLevel Vectors:  0.6859496650371175\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors:  0.6887561108093427\n",
      "SVM, WordLevel TF-IDF:  0.6971754481260185\n",
      "SVM, N-Gram Vectors:  0.6502806445772226\n",
      "SVM, CharLevel Vectors:  0.6969943871084555\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.6840485243527069\n",
      "RF, WordLevel TF-IDF:  0.6902045989498461\n",
      "RF, N-Gram Vectors:  0.6391453919971031\n",
      "RF, CharLevel Vectors:  0.6697447039652363\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print( \"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"RF, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# RF on Character Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"RF, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost, Count Vectors:  0.6637696903856599\n",
      "Adaboost, WordLevel TF-IDF:  0.6618685497012493\n",
      "Adaboost, N-Gram Vectors:  0.5816585189208764\n",
      "Adaboost, CharLevel Vectors:  0.6606011225783089\n"
     ]
    }
   ],
   "source": [
    "# Adaboost on Count Vectors\n",
    "accuracy = train_model(ensemble.AdaBoostClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print( \"Adaboost, Count Vectors: \", accuracy)\n",
    "\n",
    "# Adaboost on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.AdaBoostClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Adaboost, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Adaboost on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.AdaBoostClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"Adaboost, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Adaboost on Character Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.AdaBoostClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"Adaboost, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.6673003802281369\n",
      "Xgb, WordLevel TF-IDF:  0.6653087090349448\n",
      "Xgb, N-Gram Vectors:  0.5831975375701611\n",
      "Xgb, CharLevel Vectors:  0.6701973565091436\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "print( \"Xgb, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "33137/33137 [==============================] - 215s 6ms/step - loss: 6.9029\n",
      "Epoch 2/3\n",
      "33137/33137 [==============================] - 203s 6ms/step - loss: 6.9029\n",
      "Epoch 3/3\n",
      "33137/33137 [==============================] - 205s 6ms/step - loss: 6.9029\n",
      "NN, Count Vectors 0.4384392540286076\n",
      "Epoch 1/3\n",
      "33137/33137 [==============================] - 13s 398us/step - loss: 6.9029\n",
      "Epoch 2/3\n",
      "33137/33137 [==============================] - 12s 367us/step - loss: 6.9029\n",
      "Epoch 3/3\n",
      "33137/33137 [==============================] - 12s 363us/step - loss: 6.9029\n",
      "NN, WordLevel TF-IDF 0.4384392540286076\n",
      "Epoch 1/3\n",
      "33137/33137 [==============================] - 11s 331us/step - loss: 6.9029\n",
      "Epoch 2/3\n",
      "33137/33137 [==============================] - 10s 301us/step - loss: 6.9029\n",
      "Epoch 3/3\n",
      "33137/33137 [==============================] - 10s 296us/step - loss: 6.9029\n",
      "NN, Ngram Level TF IDF Vectors 0.4384392540286076\n",
      "Epoch 1/3\n",
      "33137/33137 [==============================] - 36s 1ms/step - loss: 6.9029\n",
      "Epoch 2/3\n",
      "33137/33137 [==============================] - 35s 1ms/step - loss: 6.9029\n",
      "Epoch 3/3\n",
      "33137/33137 [==============================] - 34s 1ms/step - loss: 6.9029\n",
      "NN, CharLevel Vectors 0.4384392540286076\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier_count = create_model_architecture(xtrain_count.shape[1])\n",
    "classifier_tfidf = create_model_architecture(xtrain_tfidf.shape[1])\n",
    "classifier_tfidf_ngram = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "classifier_tfidf_character = create_model_architecture(xtrain_tfidf_ngram_chars.shape[1])\n",
    "\n",
    "accuracy = train_model(classifier_count, xtrain_count, train_y, xvalid_count, is_neural_net=True)\n",
    "print(\"NN, Count Vectors\",  accuracy)\n",
    "\n",
    "accuracy = train_model(classifier_tfidf, xtrain_tfidf, train_y, xvalid_tfidf, is_neural_net=True)\n",
    "print(\"NN, WordLevel TF-IDF\",  accuracy)\n",
    "\n",
    "accuracy = train_model(classifier_tfidf_ngram, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)\n",
    "\n",
    "accuracy = train_model(classifier_tfidf_character, xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, is_neural_net=True)\n",
    "print(\"NN, CharLevel Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stacking Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Logistic Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.578, total=   0.1s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.578, total=   0.1s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=0.01, penalty=l2, score=0.577, total=   0.1s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.577, total=   0.1s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.578, total=   0.1s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.3s remaining:    0.0s\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=0.1, penalty=l2, score=0.677, total=   0.2s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.675, total=   0.2s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.671, total=   0.2s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.674, total=   0.2s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.668, total=   0.2s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, penalty=l2, score=0.688, total=   0.5s\n",
      "[CV] C=1, penalty=l2 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, penalty=l2, score=0.685, total=   0.5s\n",
      "[CV] C=1, penalty=l2 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, penalty=l2, score=0.682, total=   0.5s\n",
      "[CV] C=1, penalty=l2 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, penalty=l2, score=0.688, total=   0.5s\n",
      "[CV] C=1, penalty=l2 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, penalty=l2, score=0.680, total=   0.5s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, penalty=l2, score=0.673, total=   0.4s\n",
      "[CV] C=10, penalty=l2 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, penalty=l2, score=0.675, total=   0.5s\n",
      "[CV] C=10, penalty=l2 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, penalty=l2, score=0.671, total=   0.5s\n",
      "[CV] C=10, penalty=l2 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, penalty=l2, score=0.674, total=   0.5s\n",
      "[CV] C=10, penalty=l2 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, penalty=l2, score=0.668, total=   0.5s\n",
      "{'C': 1, 'penalty': 'l2'}\n",
      "0.693735288792323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Grid search cross validation and parameter tuning:\n",
    "logreg = LogisticRegression()\n",
    "logreg_grid = {\"C\":[0.01, 0.1, 1, 10], \"penalty\":[\"l1\",\"l2\"]} # l1 lasso l2 ridge\n",
    "logreg_cv = GridSearchCV(logreg,logreg_grid,cv=5,verbose=10)\n",
    "logreg_cv.fit(xtrain_tfidf, train_y)\n",
    "print(logreg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693735288792323\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob_logreg_cv = logreg_cv.predict_proba(xvalid_tfidf)[:, 1]\n",
    "y_pred_prob_logreg_cv\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_logreg_cv = logreg_cv.predict(xvalid_tfidf)\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(valid_y, y_pred_class_logreg_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.598, total=   2.4s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.601, total=   2.0s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.597, total=   1.9s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    6.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.594, total=   2.0s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    8.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.593, total=   1.9s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=200 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=200, score=0.594, total=   4.0s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=200 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   14.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=200, score=0.599, total=   3.9s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=200 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   18.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=200, score=0.592, total=   3.8s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=200 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   21.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=200, score=0.590, total=   3.6s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=200 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   25.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=200, score=0.594, total=   3.6s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.600, total=   9.7s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.596, total=   9.0s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.591, total=   8.8s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.596, total=   9.0s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.592, total=   9.1s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=100, score=0.597, total=   1.9s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=100, score=0.595, total=   1.9s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=100, score=0.598, total=   1.8s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=100, score=0.595, total=   1.8s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=100, score=0.591, total=   1.8s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=200, score=0.597, total=   3.6s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=200, score=0.593, total=   3.5s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=200, score=0.596, total=   3.4s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=200, score=0.588, total=   3.6s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=200, score=0.593, total=   3.8s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=500, score=0.595, total=   9.7s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=500, score=0.597, total=   9.2s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=500, score=0.593, total=   8.6s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=500, score=0.595, total=   8.9s\n",
      "[CV] max_depth=10, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=auto, n_estimators=500, score=0.590, total=   9.0s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=100, score=0.651, total=   5.0s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=100, score=0.660, total=   4.8s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=100, score=0.654, total=   4.7s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=100, score=0.645, total=   4.9s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=100, score=0.641, total=   4.8s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=200, score=0.645, total=   9.6s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=200, score=0.664, total=  10.1s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=200, score=0.656, total=  10.5s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=200, score=0.643, total=  10.4s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=200, score=0.642, total=   9.8s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=500, score=0.644, total=  24.1s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=500, score=0.666, total=  24.9s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=500, score=0.654, total=  24.4s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=500, score=0.644, total=  25.6s\n",
      "[CV] max_depth=20, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=sqrt, n_estimators=500, score=0.643, total=  23.6s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=100, score=0.647, total=   4.8s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=100, score=0.666, total=   4.9s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=100, score=0.646, total=   4.6s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=100, score=0.640, total=   4.8s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=100, score=0.644, total=   5.2s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=200, score=0.647, total=  10.0s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=200, score=0.660, total=  10.7s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=200, score=0.657, total=  10.1s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=200, score=0.642, total=  10.1s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=200, score=0.641, total=  10.3s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=500, score=0.647, total=  25.8s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=500, score=0.663, total=  25.3s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=500, score=0.654, total=  24.6s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=500, score=0.641, total=  25.7s\n",
      "[CV] max_depth=20, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=20, max_features=auto, n_estimators=500, score=0.645, total=  25.0s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=100, score=0.670, total=  18.0s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=100, score=0.683, total=  18.1s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=100, score=0.677, total=  18.2s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=100, score=0.664, total=  18.5s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=100, score=0.671, total=  19.4s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=200, score=0.679, total=  38.4s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=200, score=0.694, total=  36.7s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=200, score=0.676, total=  41.1s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=200, score=0.668, total=  38.9s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=200, score=0.667, total=  38.7s\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=500, score=0.676, total= 1.6min\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=500, score=0.690, total= 1.6min\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=500, score=0.678, total= 1.6min\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=500, score=0.667, total= 1.6min\n",
      "[CV] max_depth=50, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=sqrt, n_estimators=500, score=0.668, total= 1.6min\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=100, score=0.673, total=  19.9s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=100, score=0.683, total=  20.2s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=100, score=0.673, total=  18.6s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=100, score=0.670, total=  18.7s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=100 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=100, score=0.668, total=  19.1s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=200, score=0.675, total=  36.5s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=200, score=0.689, total=  35.6s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=200, score=0.677, total=  35.7s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=200, score=0.670, total=  35.9s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=200 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=200, score=0.666, total=  35.5s\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=500, score=0.674, total= 1.5min\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=500, score=0.691, total= 1.6min\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=500, score=0.675, total= 1.5min\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=500, score=0.667, total= 1.6min\n",
      "[CV] max_depth=50, max_features=auto, n_estimators=500 ...............\n",
      "[CV]  max_depth=50, max_features=auto, n_estimators=500, score=0.670, total= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 34.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.693735288792323\n"
     ]
    }
   ],
   "source": [
    "# import and instantiate a logistic regression model\n",
    "rf = RandomForestClassifier()\n",
    "rf_grid = {'n_estimators': [100, 200, 500],\n",
    "           'max_features': ['sqrt','auto'],\n",
    "           'max_depth': [10, 20, 50]}\n",
    "rf_cv = GridSearchCV(rf,rf_grid,cv=5,verbose=10)\n",
    "rf_cv.fit(xtrain_tfidf, train_y)\n",
    "print(rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6794314684048525\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob_RandomForest = rf_cv.predict_proba(xvalid_tfidf)[:, 1]\n",
    "y_pred_prob_RandomForest\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_RandomForest = rf_cv.predict(xvalid_tfidf)\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(valid_y, y_pred_class_RandomForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. XGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=100 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=100, score=0.658, total=  10.5s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=100 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=100, score=0.672, total=   9.9s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=100 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   20.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=100, score=0.664, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=100 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   30.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=100, score=0.653, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=100 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   40.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=100, score=0.654, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=200 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   49.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=200, score=0.672, total=  19.1s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=200 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=200, score=0.680, total=  19.0s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=200 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  1.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=200, score=0.677, total=  19.0s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=200 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=200, score=0.666, total=  19.0s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=200 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=200, score=0.666, total=  19.0s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=500, score=0.681, total=  46.9s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=500, score=0.693, total=  46.9s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=500, score=0.685, total=  46.2s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=500, score=0.681, total=  46.2s\n",
      "[CV] gamma=0, learning_rate=0.1, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.1, n_estimators=500, score=0.678, total=  46.3s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=100, score=0.666, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=100, score=0.679, total=   9.8s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=100, score=0.672, total=   9.6s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=100, score=0.662, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=100, score=0.671, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=200, score=0.676, total=  18.9s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=200, score=0.685, total=  18.9s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=200, score=0.677, total=  18.8s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=200, score=0.675, total=  19.1s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=200, score=0.675, total=  18.8s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=500, score=0.686, total=  45.7s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=500, score=0.692, total=  46.1s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=500, score=0.686, total=  47.6s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=500, score=0.683, total=  46.7s\n",
      "[CV] gamma=0, learning_rate=0.2, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.2, n_estimators=500, score=0.685, total=  46.9s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=100, score=0.674, total=  10.0s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=100, score=0.683, total=   9.5s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=100, score=0.669, total=   9.7s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=100, score=0.672, total=   9.6s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=100 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=100, score=0.668, total=   9.5s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=200, score=0.675, total=  18.6s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=200, score=0.683, total=  18.6s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=200, score=0.674, total=  20.2s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=200, score=0.669, total=  19.5s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=200 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=200, score=0.673, total=  20.0s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=500, score=0.677, total=  49.8s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=500, score=0.683, total=  48.9s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=500, score=0.679, total=  49.4s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=500, score=0.672, total=  48.4s\n",
      "[CV] gamma=0, learning_rate=0.5, n_estimators=500 ....................\n",
      "[CV]  gamma=0, learning_rate=0.5, n_estimators=500, score=0.688, total=  48.6s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=100, score=0.658, total=  10.5s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=100, score=0.672, total=  10.2s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=100, score=0.664, total=  10.3s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=100, score=0.653, total=  10.0s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=100, score=0.654, total=   9.9s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=200, score=0.670, total=  18.9s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=200, score=0.680, total=  19.0s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=200, score=0.677, total=  19.0s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=200, score=0.666, total=  19.0s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=200, score=0.666, total=  19.5s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=500, score=0.685, total=  47.7s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=500, score=0.692, total=  51.0s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=500, score=0.684, total=  50.1s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=500, score=0.679, total=  50.1s\n",
      "[CV] gamma=0.1, learning_rate=0.1, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.1, n_estimators=500, score=0.681, total=  47.3s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=100, score=0.666, total=   9.9s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=100, score=0.677, total=  10.1s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=100, score=0.672, total=  10.5s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=100, score=0.664, total=  10.5s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=100 ..................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=100, score=0.670, total=  10.7s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=200, score=0.676, total=  18.8s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=200, score=0.688, total=  19.0s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=200, score=0.679, total=  18.9s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=200, score=0.671, total=  19.1s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=200, score=0.674, total=  19.8s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=500, score=0.689, total=  48.6s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=500, score=0.693, total=  49.7s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=500, score=0.687, total=  46.2s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=500, score=0.681, total=  46.7s\n",
      "[CV] gamma=0.1, learning_rate=0.2, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.2, n_estimators=500, score=0.686, total=  48.7s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=100, score=0.677, total=   9.7s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=100, score=0.683, total=   9.6s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=100, score=0.669, total=   9.7s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=100, score=0.671, total=  10.9s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=100 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=100, score=0.670, total=   9.9s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=200, score=0.679, total=  19.5s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=200, score=0.682, total=  19.6s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=200, score=0.675, total=  19.4s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=200, score=0.670, total=  18.8s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=200 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=200, score=0.674, total=  18.6s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=500, score=0.677, total=  46.0s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=500, score=0.684, total=  46.3s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=500, score=0.681, total=  46.8s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=500, score=0.669, total=  47.3s\n",
      "[CV] gamma=0.1, learning_rate=0.5, n_estimators=500 ..................\n",
      "[CV]  gamma=0.1, learning_rate=0.5, n_estimators=500, score=0.685, total=  47.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 38.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.1, 'learning_rate': 0.2, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_grid = {'learning_rate': [0.1, 0.2, 0.5],\n",
    "             'n_estimators': [100, 200, 500],\n",
    "             'gamma': [0, 0.1]}\n",
    "xgb_cv = GridSearchCV(xgb,xgb_grid,cv=5,verbose=10)\n",
    "xgb_cv.fit(xtrain_tfidf, train_y)\n",
    "print(xgb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700615607459714\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob_clf_XGB  = xgb_cv.predict_proba(xvalid_tfidf)[:, 1]\n",
    "y_pred_prob_clf_XGB \n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_clf_XGB = xgb_cv.predict(xvalid_tfidf)\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(valid_y, y_pred_class_clf_XGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=2017)\n",
    "num_rounds = 10\n",
    "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200)\n",
    "# Use early_stopping_rounds to stop the cv when there is no score imporovement\n",
    "adaboost.fit(xtrain_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6811515480717002\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob_adaboost  = adaboost.predict_proba(xvalid_tfidf)[:, 1]\n",
    "y_pred_prob_adaboost\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_adaboost = adaboost.predict(xvalid_tfidf)\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(valid_y, y_pred_class_adaboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. MLP NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62191475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6923773311606011"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = MLPClassifier(hidden_layer_sizes=(100,50), max_iter=1,activation = 'relu',solver='adam',random_state=1,verbose=10)\n",
    "NN.fit(xtrain_tfidf, train_y)\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob_NN = NN.predict_proba(xvalid_tfidf)[:, 1]\n",
    "y_pred_prob_NN\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_NN = NN.predict(xvalid_tfidf)\n",
    "# calculate accuracy\n",
    "metrics.accuracy_score(valid_y, y_pred_class_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. ExtraTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6943690023537932"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extratree = ExtraTreesClassifier()\n",
    "# Use early_stopping_rounds to stop the cv when there is no score imporovement\n",
    "extratree.fit(xtrain_tfidf, train_y)\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob_extratree = extratree.predict_proba(xvalid_tfidf)[:, 1]\n",
    "y_pred_prob_extratree\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_extratree = extratree.predict(xvalid_tfidf)\n",
    "# calculate accuracy\n",
    "metrics.accuracy_score(valid_y, y_pred_class_extratree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.25001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 137164\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_train[\"clean\"])\n",
    "sequences = tokenizer.texts_to_sequences(df_train[\"clean\"])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (44183, 1000)\n",
      "Shape of Label Tensor: (44183, 2)\n",
      "Shape of train Data Tensor: (33137, 1000)\n",
      "Shape of validate Data Tensor: (11046, 1000)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(df_train[\"Outcome\"]))\n",
    "print('Shape of Data Tensor:', data.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "print('Shape of train Data Tensor:', x_train.shape)\n",
    "print('Shape of validate Data Tensor:', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 50d.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 50d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding:\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/damengjin/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Simplified convolutional neural network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          6858250   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          32128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 7,071,244\n",
      "Trainable params: 7,071,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33137 samples, validate on 11046 samples\n",
      "Epoch 1/3\n",
      "33137/33137 [==============================] - 1485s 45ms/step - loss: 0.6958 - acc: 0.6180 - val_loss: 0.7022 - val_acc: 0.6018\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60176, saving model to model_cnn.hdf5\n",
      "Epoch 2/3\n",
      "33137/33137 [==============================] - 1466s 44ms/step - loss: 0.7187 - acc: 0.6550 - val_loss: 0.7293 - val_acc: 0.6466\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.60176 to 0.64657, saving model to model_cnn.hdf5\n",
      "Epoch 3/3\n",
      "33137/33137 [==============================] - 1529s 46ms/step - loss: 0.9982 - acc: 0.6466 - val_loss: 0.9718 - val_acc: 0.6582\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64657 to 0.65816, saving model to model_cnn.hdf5\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=3, batch_size=2,callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_pred_prob_result_deep_cnn = model.predict(x_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = {\n",
    "    \"RandomForest\":y_pred_prob_RandomForest,\n",
    "    \"logreg_cv\":y_pred_prob_logreg_cv,\n",
    "    \"NN\":y_pred_prob_NN,\n",
    "    \"XGB\":y_pred_prob_clf_XGB,\n",
    "    \"adaboost\":y_pred_prob_adaboost,\n",
    "    \"extratree\": y_pred_prob_extratree,\n",
    "    \"deep_cnn\": xtest_pred_prob_result_deep_cnn\n",
    "}\n",
    "df_test_pred = pd.DataFrame(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>logreg_cv</th>\n",
       "      <th>NN</th>\n",
       "      <th>XGB</th>\n",
       "      <th>adaboost</th>\n",
       "      <th>extratree</th>\n",
       "      <th>deep_cnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.713676</td>\n",
       "      <td>0.777976</td>\n",
       "      <td>0.740634</td>\n",
       "      <td>0.657034</td>\n",
       "      <td>0.501595</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.479676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.457048</td>\n",
       "      <td>0.382006</td>\n",
       "      <td>0.442490</td>\n",
       "      <td>0.359940</td>\n",
       "      <td>0.498207</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.554012</td>\n",
       "      <td>0.505793</td>\n",
       "      <td>0.461528</td>\n",
       "      <td>0.649797</td>\n",
       "      <td>0.501189</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.561898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644851</td>\n",
       "      <td>0.842044</td>\n",
       "      <td>0.808806</td>\n",
       "      <td>0.851214</td>\n",
       "      <td>0.503453</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.500754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.596983</td>\n",
       "      <td>0.565473</td>\n",
       "      <td>0.513614</td>\n",
       "      <td>0.446266</td>\n",
       "      <td>0.500414</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.496739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.467835</td>\n",
       "      <td>0.584228</td>\n",
       "      <td>0.608317</td>\n",
       "      <td>0.533376</td>\n",
       "      <td>0.499331</td>\n",
       "      <td>0.568667</td>\n",
       "      <td>0.494948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.683130</td>\n",
       "      <td>0.881192</td>\n",
       "      <td>0.849912</td>\n",
       "      <td>0.745085</td>\n",
       "      <td>0.501021</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.469123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.426103</td>\n",
       "      <td>0.310727</td>\n",
       "      <td>0.250585</td>\n",
       "      <td>0.275192</td>\n",
       "      <td>0.498028</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.457482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.603898</td>\n",
       "      <td>0.845299</td>\n",
       "      <td>0.831312</td>\n",
       "      <td>0.663779</td>\n",
       "      <td>0.500685</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.593986</td>\n",
       "      <td>0.643697</td>\n",
       "      <td>0.654914</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.500448</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.596445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RandomForest  logreg_cv        NN       XGB  adaboost  extratree  deep_cnn\n",
       "0      0.713676   0.777976  0.740634  0.657034  0.501595   0.820000  0.479676\n",
       "1      0.457048   0.382006  0.442490  0.359940  0.498207   0.270000  1.000000\n",
       "2      0.554012   0.505793  0.461528  0.649797  0.501189   0.740000  0.561898\n",
       "3      0.644851   0.842044  0.808806  0.851214  0.503453   0.690000  0.500754\n",
       "4      0.596983   0.565473  0.513614  0.446266  0.500414   0.740000  0.496739\n",
       "5      0.467835   0.584228  0.608317  0.533376  0.499331   0.568667  0.494948\n",
       "6      0.683130   0.881192  0.849912  0.745085  0.501021   0.790000  0.469123\n",
       "7      0.426103   0.310727  0.250585  0.275192  0.498028   0.230000  0.457482\n",
       "8      0.603898   0.845299  0.831312  0.663779  0.500685   0.870000  1.000000\n",
       "9      0.593986   0.643697  0.654914  0.507463  0.500448   0.590000  0.596445"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use variables in the outer function instead of...</td>\n",
       "      <td>68045</td>\n",
       "      <td>us vary out funct instead glob vary get best a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you're looking for something as nice as pyt...</td>\n",
       "      <td>60790</td>\n",
       "      <td>look someth nic python x  not think luck stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i use the tail() function: tail(vector, n=1) t...</td>\n",
       "      <td>53896</td>\n",
       "      <td>us tail funct tail vect n1 nic thing tail work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clearly i should have worked on this for anoth...</td>\n",
       "      <td>50204</td>\n",
       "      <td>clear work anoth hour post quest obvy retrospe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you are, indeed, passing the object around and...</td>\n",
       "      <td>60771</td>\n",
       "      <td>indee pass object around us mem think op objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>most of the algorithms for eigen value computa...</td>\n",
       "      <td>77143</td>\n",
       "      <td>algorithm eig valu comput scal bigoh n3 n rowc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>see tip 7 about adjusting the margins. excerpt...</td>\n",
       "      <td>69808</td>\n",
       "      <td>see tip  adjust margin excerpt remov spac rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>see ?which.max &gt; which.max( matrix[,2] ) [1] 2</td>\n",
       "      <td>78092</td>\n",
       "      <td>see whichmax whichmax matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rolling means/maximums/medians in the zoo pack...</td>\n",
       "      <td>64047</td>\n",
       "      <td>rol meansmaximumsm zoo pack rollm movingav ttr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>for no good reason i'm aware of, dev.off(), un...</td>\n",
       "      <td>73281</td>\n",
       "      <td>good reason aw devoff unlik dev rel funct lik ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment     Id  \\\n",
       "0  use variables in the outer function instead of...  68045   \n",
       "1  if you're looking for something as nice as pyt...  60790   \n",
       "2  i use the tail() function: tail(vector, n=1) t...  53896   \n",
       "3  clearly i should have worked on this for anoth...  50204   \n",
       "4  you are, indeed, passing the object around and...  60771   \n",
       "5  most of the algorithms for eigen value computa...  77143   \n",
       "6  see tip 7 about adjusting the margins. excerpt...  69808   \n",
       "7     see ?which.max > which.max( matrix[,2] ) [1] 2  78092   \n",
       "8  rolling means/maximums/medians in the zoo pack...  64047   \n",
       "9  for no good reason i'm aware of, dev.off(), un...  73281   \n",
       "\n",
       "                                               clean  \n",
       "0  us vary out funct instead glob vary get best a...  \n",
       "1  look someth nic python x  not think luck stand...  \n",
       "2  us tail funct tail vect n1 nic thing tail work...  \n",
       "3  clear work anoth hour post quest obvy retrospe...  \n",
       "4  indee pass object around us mem think op objec...  \n",
       "5  algorithm eig valu comput scal bigoh n3 n rowc...  \n",
       "6  see tip  adjust margin excerpt remov spac rese...  \n",
       "7                       see whichmax whichmax matrix  \n",
       "8  rol meansmaximumsm zoo pack rollm movingav ttr...  \n",
       "9  good reason aw devoff unlik dev rel funct lik ...  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate preprocess steps for test data:\n",
    "df_test[\"clean\"] = df_test.Comment.apply(replace_contractions)\n",
    "df_test[\"clean\"]  = df_test[\"clean\"].apply(nltk.word_tokenize)\n",
    "df_test[\"clean\"]  = df_test[\"clean\"].apply(normalize)\n",
    "df_test[\"clean\"]  = df_test[\"clean\"].apply(stem_and_lemmatize)\n",
    "df_test[\"clean\"]  = df_test[\"clean\"].apply(TreebankWordDetokenizer().detokenize)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "test_tfidf =  tfidf_vect.transform(df_test[\"clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_prob_result = nb.predict_proba(X_result_dtm)[:, 1]\n",
    "y_pred_prob_result_RandomForest = rf_cv.predict_proba(test_tfidf)[:, 1]\n",
    "y_pred_prob_result_logreg_cv = logreg_cv.predict_proba(test_tfidf)[:, 1]\n",
    "y_pred_prob_result_NN = NN.predict_proba(test_tfidf)[:, 1]\n",
    "y_pred_prob_result_clf_XGB = xgb_cv.predict_proba(test_tfidf)[:, 1]\n",
    "y_pred_prob_result_adaboost = adaboost.predict_proba(test_tfidf)[:, 1]\n",
    "y_pred_prob_result_extratree = extratree.predict_proba(test_tfidf)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (28200, 1000)\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(df_test[\"clean\"])\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of Data Tensor:', data_test.shape)\n",
    "y_pred_prob_result_deep_cnn = model.predict(data_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_result = {\n",
    "    \"RandomForest\":y_pred_prob_result_RandomForest,\n",
    "    \"logreg_cv\":y_pred_prob_result_logreg_cv,\n",
    "    \"NN\":y_pred_prob_result_NN,\n",
    "    \"XGB\":y_pred_prob_result_clf_XGB,\n",
    "    \"adaboost\":y_pred_prob_result_adaboost,\n",
    "    \"extratree\":y_pred_prob_result_extratree,\n",
    "    \"deep_cnn\":y_pred_prob_result_deep_cnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>logreg_cv</th>\n",
       "      <th>NN</th>\n",
       "      <th>XGB</th>\n",
       "      <th>adaboost</th>\n",
       "      <th>extratree</th>\n",
       "      <th>deep_cnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534480</td>\n",
       "      <td>0.779456</td>\n",
       "      <td>0.754049</td>\n",
       "      <td>0.633159</td>\n",
       "      <td>0.500919</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.545244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.565033</td>\n",
       "      <td>0.702514</td>\n",
       "      <td>0.677042</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.501089</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.578214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.536936</td>\n",
       "      <td>0.624570</td>\n",
       "      <td>0.637022</td>\n",
       "      <td>0.657852</td>\n",
       "      <td>0.501919</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.520217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545158</td>\n",
       "      <td>0.618712</td>\n",
       "      <td>0.674909</td>\n",
       "      <td>0.396508</td>\n",
       "      <td>0.501036</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.590322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.809380</td>\n",
       "      <td>0.851264</td>\n",
       "      <td>0.846711</td>\n",
       "      <td>0.764513</td>\n",
       "      <td>0.502041</td>\n",
       "      <td>0.840</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.675821</td>\n",
       "      <td>0.340298</td>\n",
       "      <td>0.345103</td>\n",
       "      <td>0.487131</td>\n",
       "      <td>0.500894</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.614686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.611977</td>\n",
       "      <td>0.781798</td>\n",
       "      <td>0.790784</td>\n",
       "      <td>0.733990</td>\n",
       "      <td>0.503667</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.549541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.513002</td>\n",
       "      <td>0.435319</td>\n",
       "      <td>0.375095</td>\n",
       "      <td>0.489146</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.505566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.482264</td>\n",
       "      <td>0.395722</td>\n",
       "      <td>0.350307</td>\n",
       "      <td>0.325369</td>\n",
       "      <td>0.497271</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.500445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.532100</td>\n",
       "      <td>0.778034</td>\n",
       "      <td>0.783298</td>\n",
       "      <td>0.499852</td>\n",
       "      <td>0.500265</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.541041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RandomForest  logreg_cv        NN       XGB  adaboost  extratree  deep_cnn\n",
       "0      0.534480   0.779456  0.754049  0.633159  0.500919      0.500  0.545244\n",
       "1      0.565033   0.702514  0.677042  0.753000  0.501089      0.680  0.578214\n",
       "2      0.536936   0.624570  0.637022  0.657852  0.501919      0.650  0.520217\n",
       "3      0.545158   0.618712  0.674909  0.396508  0.501036      0.490  0.590322\n",
       "4      0.809380   0.851264  0.846711  0.764513  0.502041      0.840  1.000000\n",
       "5      0.675821   0.340298  0.345103  0.487131  0.500894      0.620  0.614686\n",
       "6      0.611977   0.781798  0.790784  0.733990  0.503667      0.520  0.549541\n",
       "7      0.513002   0.435319  0.375095  0.489146  0.500237      0.564  0.505566\n",
       "8      0.482264   0.395722  0.350307  0.325369  0.497271      0.424  0.500445\n",
       "9      0.532100   0.778034  0.783298  0.499852  0.500265      0.650  0.541041"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_result = pd.DataFrame(predictions_result)\n",
    "df_pred_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28200, 7)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8284, 7)\n",
      "(2762, 7)\n"
     ]
    }
   ],
   "source": [
    "# Train the Stacking model:\n",
    "#split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_result_train, X_result_test, y_result_train, y_result_test = train_test_split(df_test_pred, valid_y, random_state=1)\n",
    "print(X_result_train.shape)\n",
    "print(X_result_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] ................. C=0.001, penalty=l2, score=0.640, total=   0.1s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] ................. C=0.001, penalty=l2, score=0.626, total=   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] ................. C=0.001, penalty=l2, score=0.649, total=   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] ................. C=0.001, penalty=l2, score=0.639, total=   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] ................. C=0.001, penalty=l2, score=0.651, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.700, total=   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.705, total=   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=0.01, penalty=l2, score=0.716, total=   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.710, total=   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] .................. C=0.01, penalty=l2, score=0.708, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.700, total=   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.709, total=   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.710, total=   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.719, total=   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ................... C=0.1, penalty=l2, score=0.718, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.701, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.704, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1.0, penalty=l2, score=0.709, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.721, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.719, total=   0.0s\n",
      "[CV] C=10.0, penalty=l1 ..............................................\n",
      "[CV] .................... C=10.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10.0, penalty=l1 ..............................................\n",
      "[CV] .................... C=10.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10.0, penalty=l1 ..............................................\n",
      "[CV] .................... C=10.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10.0, penalty=l1 ..............................................\n",
      "[CV] .................... C=10.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10.0, penalty=l1 ..............................................\n",
      "[CV] .................... C=10.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10.0, penalty=l2 ..............................................\n",
      "[CV] .................. C=10.0, penalty=l2, score=0.701, total=   0.0s\n",
      "[CV] C=10.0, penalty=l2 ..............................................\n",
      "[CV] .................. C=10.0, penalty=l2, score=0.705, total=   0.0s\n",
      "[CV] C=10.0, penalty=l2 ..............................................\n",
      "[CV] .................. C=10.0, penalty=l2, score=0.710, total=   0.0s\n",
      "[CV] C=10.0, penalty=l2 ..............................................\n",
      "[CV] .................. C=10.0, penalty=l2, score=0.721, total=   0.0s\n",
      "[CV] C=10.0, penalty=l2 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damengjin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10.0, penalty=l2, score=0.719, total=   0.0s\n",
      "tuned hpyerparameters :(best parameters)  {'C': 10.0, 'penalty': 'l2'}\n",
      "accuracy : 0.7113722634759868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid={\"C\":np.logspace(-3,1,5), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg_cv_result=GridSearchCV(logreg,grid,cv=5,verbose=10)\n",
    "logreg_cv_result.fit(X_result_train, y_result_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv_result.best_params_)\n",
    "print(\"accuracy :\",logreg_cv_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_prob = logreg_cv_result.predict(df_pred_result)\n",
    "import numpy as np\n",
    "\n",
    "y_result_pred_class_ensemble = []\n",
    "for i in range(len(new_pred_prob)):\n",
    "    if new_pred_prob [i] > 0.5:\n",
    "        y_result_pred_class_ensemble.append(1)\n",
    "    else:\n",
    "        y_result_pred_class_ensemble.append(0)\n",
    "\n",
    "y_result_pred_class_ensemble = np.array(y_result_pred_class_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame ({\n",
    "    \"Id\": df_test.Id,\n",
    "    \"Outcome\": y_result_pred_class_ensemble\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Outcome\n",
       "0  68045        1\n",
       "1  60790        1\n",
       "2  53896        1\n",
       "3  50204        1\n",
       "4  60771        1"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('20200327_Stacking_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
